{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../data/POS/raw/new_sec_01-22.tagged\"\n",
    "val_file = \"../data/POS/raw/new_sec_00.tagged\"\n",
    "test_file = \"../data/POS/raw/new_sec_00.tagged\"\n",
    "block_size = 102\n",
    "sentence_tokens = []\n",
    "test_sentence_tokens = []\n",
    "sentence_POS = []\n",
    "val_sentence_tokens = []\n",
    "val_sentence_POS = []\n",
    "test_sentence_POS = []\n",
    "removed_sentences = []\n",
    "vocab = []\n",
    "vocab_POS = []\n",
    "x = open(train_file)\n",
    "for line in x.readlines():\n",
    "    tokens = []\n",
    "    POS = []\n",
    "    pairs = line.split(\" \")\n",
    "    if len(pairs) <= block_size-2:\n",
    "        for pair in pairs:\n",
    "            pair = pair.strip('\\n')\n",
    "            if pair != \"\\n\":\n",
    "                word = pair.split(\"|\")[0]\n",
    "                tag = pair.split(\"|\")[1]\n",
    "                tokens.append(word)\n",
    "                vocab.append(word)\n",
    "                vocab_POS.append(tag)\n",
    "                POS.append(tag)\n",
    "        sentence_tokens.append(tokens)\n",
    "        sentence_POS.append(POS)\n",
    "    else:\n",
    "        removed_sentences.append([pair.split(\"|\")[0] for pair in pairs])\n",
    "x.close()\n",
    "\n",
    "x = open(val_file)\n",
    "for line in x.readlines():\n",
    "    tokens = []\n",
    "    POS = []\n",
    "    pairs = line.split(\" \")\n",
    "    if len(pairs) <= block_size-2:\n",
    "        for pair in pairs:\n",
    "            pair = pair.strip('\\n')\n",
    "            if pair != \"\\n\":\n",
    "                word = pair.split(\"|\")[0]\n",
    "                tag = pair.split(\"|\")[1]\n",
    "                tokens.append(word)\n",
    "                vocab.append(word)\n",
    "                vocab_POS.append(tag)\n",
    "                POS.append(tag)\n",
    "        val_sentence_tokens.append(tokens)\n",
    "        val_sentence_POS.append(POS)\n",
    "x.close()\n",
    "\n",
    "x = open(test_file)\n",
    "for line in x.readlines():\n",
    "    tokens = []\n",
    "    POS = []\n",
    "    pairs = line.split(\" \")\n",
    "    if len(pairs) <= block_size-2:\n",
    "        for pair in pairs:\n",
    "            pair = pair.strip('\\n')\n",
    "            if pair != \"\\n\":\n",
    "                word = pair.split(\"|\")[0]\n",
    "                tag = pair.split(\"|\")[1]\n",
    "                tokens.append(word)\n",
    "                vocab.append(word)\n",
    "                vocab_POS.append(tag)\n",
    "                POS.append(tag)\n",
    "        test_sentence_tokens.append(tokens)\n",
    "        test_sentence_POS.append(POS)\n",
    "x.close()\n",
    "\n",
    "# create mapping for POS tags\n",
    "vocab_POS = sorted(list(set(vocab_POS)))\n",
    "POS_to_idx = {tag: i+1 for i, tag in enumerate(vocab_POS)}\n",
    "\n",
    "# add padding, unknown, and start tokens\n",
    "POS_to_idx[\"<PAD>\"] = 0\n",
    "POS_to_idx[\"<START>\"] = len(POS_to_idx)\n",
    "POS_to_idx[\"<UNK>\"] = len(POS_to_idx)\n",
    "POS_to_idx[\"<END>\"] = len(POS_to_idx)\n",
    "idx_to_POS = {k: v for v, k in POS_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of sentences removed:  0.009190120620333142 %\n",
      "Number of removed sentences:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion of sentences removed: \", 100*len(removed_sentences)/(len(removed_sentences)+len(sentence_tokens)), \"%\")\n",
    "print(\"Number of removed sentences: \", len(removed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'a', 'while', 'in', 'the', '1970s', 'it', 'seemed', 'Mr.', 'Moon', 'was', 'on', 'a', 'spending', 'spree', ',', 'with', 'such', 'purchases', 'as', 'the', 'former', 'New', 'Yorker', 'Hotel', 'and', 'its', 'adjacent', 'Manhattan', 'Center', ';', 'a', 'fishing\\\\/processing', 'conglomerate', 'with', 'branches', 'in', 'Alaska', ',', 'Massachusetts', ',', 'Virginia', 'and', 'Louisiana', ';', 'a', 'former', 'Christian', 'Brothers', 'monastery', 'and', 'the', 'Seagram', 'family', 'mansion', '-LRB-', 'both', 'picturesquely', 'situated', 'on', 'the', 'Hudson', 'River', '-RRB-', ';', 'shares', 'in', 'banks', 'from', 'Washington', 'to', 'Uruguay', ';', 'a', 'motion', 'picture', 'production', 'company', ',', 'and', 'newspapers', ',', 'such', 'as', 'the', 'Washington', 'Times', ',', 'the', 'New', 'York', 'City', 'Tribune', '-LRB-', 'originally', 'the', 'News', 'World', '-RRB-', ',', 'and', 'the', 'successful', 'Spanish-language', 'Noticias', 'del', 'Mundo', '.']\n",
      "['They', 'transferred', 'some', '$', '28', 'million', 'from', 'the', 'Community', 'Development', 'Block', 'Grant', 'program', 'designated', 'largely', 'for', 'low', '-', 'and', 'moderate-income', 'projects', 'and', 'funneled', 'it', 'into', 'such', 'items', 'as', ':', '--', '$', '1.2', 'million', 'for', 'a', 'performing-arts', 'center', 'in', 'Newark', ',', '--', '$', '1.3', 'million', 'for', '``', 'job', 'retention', \"''\", 'in', 'Hawaiian', 'sugar', 'mills', '.', '--', '$', '400,000', 'for', 'a', 'collapsing', 'utility', 'tunnel', 'in', 'Salisbury', ',', '--', '$', '500,000', 'for', '``', 'equipment', 'and', 'landscaping', 'to', 'deter', 'crime', 'and', 'aid', 'police', 'surveillance', \"''\", 'at', 'a', 'Michigan', 'park', '.', '--', '$', '450,000', 'for', '``', 'integrated', 'urban', 'data', 'based', 'in', 'seven', 'cities', '.', \"''\", 'No', 'other', 'details', '.', '--', '$', '390,000', 'for', 'a', 'library', 'and', 'recreation', 'center', 'at', 'Mackinac', 'Island', ',', 'Mich', '.']\n",
      "['4', '.', 'When', 'a', 'RICO', 'TRO', 'is', 'being', 'sought', ',', 'the', 'prosecutor', 'is', 'required', ',', 'at', 'the', 'earliest', 'appropriate', 'time', ',', 'to', 'state', 'publicly', 'that', 'the', 'government', \"'s\", 'request', 'for', 'a', 'TRO', ',', 'and', 'eventual', 'forfeiture', ',', 'is', 'made', 'in', 'full', 'recognition', 'of', 'the', 'rights', 'of', 'third', 'parties', '--', 'that', 'is', ',', 'in', 'requesting', 'the', 'TRO', ',', 'the', 'government', 'will', 'not', 'seek', 'to', 'disrupt', 'the', 'normal', ',', 'legitimate', 'business', 'activities', 'of', 'the', 'defendant', ';', 'will', 'not', 'seek', 'through', 'use', 'of', 'the', 'relation-back', 'doctrine', 'to', 'take', 'from', 'third', 'parties', 'assets', 'legitimately', 'transferred', 'to', 'them', ';', 'will', 'not', 'seek', 'to', 'vitiate', 'legitimate', 'business', 'transactions', 'occurring', 'between', 'the', 'defendant', 'and', 'third', 'parties', ';', 'and', 'will', ',', 'in', 'all', 'other', 'respects', ',', 'assist', 'the', 'court', 'in', 'ensuring', 'that', 'the', 'rights', 'of', 'third', 'parties', 'are', 'protected', ',', 'through', 'proceeding', 'under', '-LCB-', 'RICO', '-RCB-', 'and', 'otherwise', '.']\n",
      "['It', 'would', 'like', 'to', 'peg', 'the', 'ceiling', 'on', 'Federal', 'Housing', 'Administration', 'mortgage', 'guarantees', 'to', '95', '%', 'of', 'the', 'median', 'price', 'in', 'a', 'particular', 'market', ',', 'instead', 'of', 'limiting', 'it', 'to', '$', '101,250', ';', 'reduce', '-LRB-', 'or', 'even', 'eliminate', '-RRB-', 'FHA', 'down-payment', 'requirements', 'and', 'increase', 'the', 'availability', 'of', 'variable-rate', 'mortgages', ';', 'expand', 'the', 'Veterans', 'Affairs', 'Department', 'loan', 'guarantee', 'program', ';', 'provide', '``', 'adequate', \"''\", 'funding', 'for', 'the', 'Farmers', 'Home', 'Administration', '-LRB-', 'FmHA', '-RRB-', ';', 'increase', 'federal', 'funding', 'and', 'tax', 'incentives', 'for', 'the', 'construction', 'of', 'low-income', 'and', 'rental', 'housing', ',', 'including', '$', '4', 'billion', 'in', 'block', 'grants', 'to', 'states', 'and', 'localities', ';', 'and', '``', 'fully', 'fund', \"''\", 'the', 'McKinney', 'Act', ',', 'a', '$', '656', 'million', 'potpourri', 'for', 'the', 'homeless', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in removed_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# change working directory to the parent directory\n",
    "parent_directory = os.path.abspath(\"..\")\n",
    "current_directory = os.getcwd()\n",
    "if current_directory != parent_directory:\n",
    "    os.chdir(parent_directory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.utils.utils import load_config, resume_training, train\n",
    "tagging = \"POS\"\n",
    "config = load_config(\"config/TI_config.yaml\")\n",
    "model_package = resume_training(model_path = f\"models/TagInsert_{tagging}\", config = config, model_name = \"TI\", tagging=tagging)\n",
    "model = train(model_package, config, tagging, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbais_giacomo\u001b[0m (\u001b[33mgbais\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4198ce9396d540438bf4264e2eb2e86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\\wandb\\run-20250124_125726-sztrtd6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gbais/TagInsert/runs/sztrtd6c' target=\"_blank\">VanillaTransformer_POS</a></strong> to <a href='https://wandb.ai/gbais/TagInsert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gbais/TagInsert' target=\"_blank\">https://wandb.ai/gbais/TagInsert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gbais/TagInsert/runs/sztrtd6c' target=\"_blank\">https://wandb.ai/gbais/TagInsert/runs/sztrtd6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bais_\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/5440 [00:00<?, ?it/s]c:\\Users\\bais_\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "train:   1%|          | 47/5440 [01:59<3:49:16,  2.55s/it, loss=2.53]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\\src\\utils\\utils.py\", line 708, in train\n",
      "    train_losses = run_epoch_VT(epoch_data_iter, model, LossCompute_VT(model.generator, criterion), model_opt, lr_scheduler, config, len_train, mode = 'train', train_state=train_state)[2]\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\\src\\utils\\utils.py\", line 596, in run_epoch_VT\n",
      "    for i, batch in enumerate(pbar):\n",
      "  File \"c:\\Users\\bais_\\anaconda3\\Lib\\site-packages\\tqdm\\std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\\src\\utils\\utils.py\", line 580, in dataload\n",
      "    embs = extract_BERT_embs(original_sentences, bert_model, tokenizer, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bais_\\source\\repos\\TagInsert-Paper\\src\\utils\\utils.py\", line 268, in extract_BERT_embs\n",
      "    hidden_states = hidden_states.to('cpu')\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a157a975eac42bdbdcf3ede8b223b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>▇█▇▆▅▅▄▆▄▄▄▄▄▄▃▃▃▃▂▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch train loss</td><td>2.52624</td></tr><tr><td>train_step</td><td>47</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">VanillaTransformer_POS</strong> at: <a href='https://wandb.ai/gbais/TagInsert/runs/sztrtd6c' target=\"_blank\">https://wandb.ai/gbais/TagInsert/runs/sztrtd6c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250124_125726-sztrtd6c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.utils.utils import load_config, resume_training, train\n",
    "tagging = \"POS\"\n",
    "config = load_config(\"config/VT_config.yaml\")\n",
    "model_package = resume_training(model_path = f\"models/VanillaTransformer_{tagging}\", config = config, model_name = \"VT\", tagging=tagging)\n",
    "model = train(model_package, config, tagging, save = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
