{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031242d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_p_value(p1, p2, n):\n",
    "    SE = ((p1*(1-p1)/n) + (p2*(1-p2)/n))**0.5\n",
    "    z = (p2-p1)/SE\n",
    "    p = 2*(1-norm.cdf(abs(z)))\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bd31e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5973713819971624)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_p_value(0.9291, 0.9350, 1015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87360c0c",
   "metadata": {},
   "source": [
    "# Wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c11696",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"CCG\"\n",
    "prop = \"1\"\n",
    "prop_to_prob ={\"1\": \"100%\", \"0.5\": \"50%\", \"0.1\": \"10%\", \"0.25\": \"25%\"}\n",
    "model_to_compare = \"TagInsertL2R\"\n",
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6eb4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# go to parent directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8c92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the files\n",
    "TI_preds = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "M2C_preds = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "TI_accs = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "M2C_accs = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "\n",
    "runs = [\"Run1\", \"Run2\", \"Run3\"]\n",
    "\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    TI_file = f\"predictions/{task}/predictions_TagInsert_{lang}_{prop}_{run}.csv\"\n",
    "    m2c_file = f\"predictions/{task}/predictions_{model_to_compare}_{lang}_{prop}_{run}.csv\"\n",
    "    with open(TI_file) as f:\n",
    "        # get number of lines in the file\n",
    "        rows = sum(1 for line in f)\n",
    "    with open(TI_file) as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            sentence = []\n",
    "            preds = line.strip().split(\", \")\n",
    "            # remove the last comma\n",
    "            # preds[-1] = preds[-1][:-1]\n",
    "            for j, pred in enumerate(preds):\n",
    "                if j == len(preds) - 1 and not pred.startswith(\"Sentence\"):\n",
    "                    # remove the last comma\n",
    "                    pred = pred[:-1]\n",
    "                if pred.startswith(\"Sentence\"):\n",
    "                    TI_accs[run].append(pred.split(\": \")[1])\n",
    "                elif i != rows - 1:\n",
    "                    word, gold, prediction, order = pred.split(\"|\")\n",
    "                    sentence.append((word, gold, prediction, order))\n",
    "            if len(sentence) != 0:\n",
    "                TI_preds[run].append(sentence)\n",
    "\n",
    "    with open(m2c_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sentence = []\n",
    "            preds = line.strip().split(\", \")\n",
    "            # remove the last comma\n",
    "            # preds[-1] = preds[-1][:-1]\n",
    "            for j, pred in enumerate(preds):\n",
    "                if j == len(preds) - 1 and not pred.startswith(\"Sentence\") and model_to_compare != \"BERT_Encoder\":\n",
    "                    # remove the last comma\n",
    "                    pred = pred[:-1]\n",
    "                if pred.startswith(\"Sentence\"):\n",
    "                    M2C_accs[run].append(pred.split(\": \")[1])\n",
    "                elif i != rows - 1:\n",
    "                    word, gold, prediction = pred.split(\"|\")\n",
    "                    sentence.append((word, gold, prediction))\n",
    "            if len(sentence) != 0:\n",
    "                M2C_preds[run].append(sentence)\n",
    "\n",
    "# convert all the accuracies to floats\n",
    "for run in runs:\n",
    "    TI_accs[run] = [float(acc) for acc in TI_accs[run]]\n",
    "    M2C_accs[run] = [float(acc) for acc in M2C_accs[run]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4f474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43046 43018 45727 0.9413694316268287 0.9407571019310255\n",
      "43024 42942 45727 0.940888315437269 0.9390950641852734\n",
      "43036 42907 45727 0.9411507424497562 0.9383296520655193\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    correct_ti = 0\n",
    "    correct_m2c = 0\n",
    "    total = 0\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        sentence_len = len(ti_preds)\n",
    "        total += sentence_len\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if ti_pred == gold:\n",
    "                correct_ti += 1\n",
    "            if m2c_pred == gold:\n",
    "                correct_m2c += 1\n",
    "\n",
    "    print(correct_ti, correct_m2c, total, correct_ti / total, correct_m2c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b73f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon statistic: 2639600.0\n",
      "P-value: 4.815640410639283e-06\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "TI_runs = np.zeros((total, len(runs)), dtype=int)\n",
    "M2C_runs = np.zeros((total, len(runs)), dtype=int)\n",
    "for run in runs:\n",
    "    run_id = 0\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if ti_pred == gold:\n",
    "                TI_runs[run_id, runs.index(run)] = 1\n",
    "            else:\n",
    "                TI_runs[run_id, runs.index(run)] = 0\n",
    "            if m2c_pred == gold:\n",
    "                M2C_runs[run_id, runs.index(run)] = 1\n",
    "            else:\n",
    "                M2C_runs[run_id, runs.index(run)] = 0\n",
    "            run_id += 1\n",
    "            \n",
    "\n",
    "# Step 1: Average accuracy per test instance across the 3 runs\n",
    "model_a_mean = TI_runs.mean(axis=1)\n",
    "model_b_mean = M2C_runs.mean(axis=1)\n",
    "\n",
    "# Step 2: Compute per-sample difference\n",
    "diffs = model_a_mean - model_b_mean\n",
    "\n",
    "# Step 3: Run Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(diffs)\n",
    "\n",
    "print(f\"Wilcoxon statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ca50c",
   "metadata": {},
   "source": [
    "# Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4e5c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def permutation_test_multiple_runs(y_true, y_preds_1, y_preds_2, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to compare the accuracy of two models over multiple runs\n",
    "    on the same dataset, using per-run permutation of predictions.\n",
    "\n",
    "    Args:\n",
    "    - y_true (array-like): The true labels (ground truth) for the test set.\n",
    "    - y_preds_1 (list of array-like): List of predictions from model 1 across runs.\n",
    "    - y_preds_2 (list of array-like): List of predictions from model 2 across runs.\n",
    "    - n_iterations (int): Number of permutations.\n",
    "\n",
    "    Returns:\n",
    "    - observed_diff (float): Observed mean accuracy difference (model 1 - model 2).\n",
    "    - p_value (float): P-value for the permutation test.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_preds_1 = [np.array(p) for p in y_preds_1]\n",
    "    y_preds_2 = [np.array(p) for p in y_preds_2]\n",
    "\n",
    "    assert len(y_preds_1) == len(y_preds_2), \"Must have same number of runs for both models\"\n",
    "    \n",
    "    n_runs = len(y_preds_1)\n",
    "\n",
    "    # Calculate observed accuracy difference\n",
    "    accs_1 = [np.mean(p == y_true) for p in y_preds_1]\n",
    "    accs_2 = [np.mean(p == y_true) for p in y_preds_2]\n",
    "    observed_diff = np.mean(accs_1) - np.mean(accs_2)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        perm_accs_1 = []\n",
    "        perm_accs_2 = []\n",
    "        for i in range(n_runs):\n",
    "            if np.random.rand() < 0.5:\n",
    "                # Keep original assignment\n",
    "                perm_accs_1.append(np.mean(y_preds_1[i] == y_true))\n",
    "                perm_accs_2.append(np.mean(y_preds_2[i] == y_true))\n",
    "            else:\n",
    "                # Swap predictions\n",
    "                perm_accs_1.append(np.mean(y_preds_2[i] == y_true))\n",
    "                perm_accs_2.append(np.mean(y_preds_1[i] == y_true))\n",
    "\n",
    "        perm_diff = np.mean(perm_accs_1) - np.mean(perm_accs_2)\n",
    "\n",
    "        if np.abs(perm_diff) >= np.abs(observed_diff):\n",
    "            count += 1\n",
    "\n",
    "    p_value = count / n_iterations\n",
    "    return observed_diff, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4ec1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds = []\n",
    "TI_labs = [[] for _ in range(len(runs))]\n",
    "M2C_labs = [[] for _ in range(len(runs))]\n",
    "for run in runs:\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if run == \"Run1\":\n",
    "                golds.append(gold)\n",
    "            TI_labs[runs.index(run)].append(ti_pred)\n",
    "            M2C_labs[runs.index(run)].append(m2c_pred)\n",
    "golds = np.array(golds)\n",
    "for run in runs:\n",
    "    TI_labs[runs.index(run)] = np.array(TI_labs[runs.index(run)])\n",
    "    M2C_labs[runs.index(run)] = np.array(M2C_labs[runs.index(run)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1873f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy: 0.0017\n",
      "P-value: 0.2444\n"
     ]
    }
   ],
   "source": [
    "observed_diff, p_value = permutation_test_multiple_runs(golds, TI_labs, M2C_labs, n_iterations=5000)\n",
    "\n",
    "# Print results\n",
    "print(f\"Observed difference in accuracy: {observed_diff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a7817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def permutation_test(y_true, y_pred_1, y_pred_2, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to compare the accuracy of two models on the same dataset.\n",
    "\n",
    "    Args:\n",
    "    - y_true (array-like): The true labels.\n",
    "    - y_pred_1 (array-like): Predictions of model 1.\n",
    "    - y_pred_2 (array-like): Predictions of model 2.\n",
    "    - n_iterations (int): Number of permutations.\n",
    "\n",
    "    Returns:\n",
    "    - observed_diff (float): Observed accuracy difference.\n",
    "    - p_value (float): P-value from permutation test.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_1 = np.array(y_pred_1)\n",
    "    y_pred_2 = np.array(y_pred_2)\n",
    "\n",
    "    # Compute observed accuracy difference\n",
    "    acc_1 = np.mean(y_pred_1 == y_true)\n",
    "    acc_2 = np.mean(y_pred_2 == y_true)\n",
    "    observed_diff = acc_1 - acc_2\n",
    "\n",
    "    count = 0\n",
    "    for _ in range(n_iterations):\n",
    "        # Randomly swap predictions\n",
    "        swap_mask = np.random.rand(len(y_true)) < 0.5\n",
    "        permuted_pred_1 = np.where(swap_mask, y_pred_2, y_pred_1)\n",
    "        permuted_pred_2 = np.where(swap_mask, y_pred_1, y_pred_2)\n",
    "\n",
    "        # Compute accuracy difference after permutation\n",
    "        acc_1_perm = np.mean(permuted_pred_1 == y_true)\n",
    "        acc_2_perm = np.mean(permuted_pred_2 == y_true)\n",
    "        perm_diff = acc_1_perm - acc_2_perm\n",
    "\n",
    "        # Count if the permuted difference is at least as extreme\n",
    "        if np.abs(perm_diff) >= np.abs(observed_diff):\n",
    "            count += 1\n",
    "\n",
    "    p_value = count / n_iterations\n",
    "    return observed_diff, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeeca936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy: 0.0006\n",
      "P-value: 0.5288\n",
      "Observed difference in accuracy: 0.0018\n",
      "P-value: 0.0622\n",
      "Observed difference in accuracy: 0.0028\n",
      "P-value: 0.0018\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(runs)):\n",
    "    observed_diff, p_value = permutation_test(golds, TI_labs[i], M2C_labs[i], n_iterations=5000)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Observed difference in accuracy: {observed_diff:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2bf7b",
   "metadata": {},
   "source": [
    "# Long Tail Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "\n",
    "if task == \"PMB\":\n",
    "    tgt_to_idx = json.load(open(f\"data/{task}/{lang}/processed/{task}_to_idx.json\"))\n",
    "    idx_to_tgt = json.load(open(f\"data/{task}/{lang}/processed/idx_to_{task}.json\"))\n",
    "    with open(f\"data/{task}/{lang}/processed/word_to_idx.json\", 'r', encoding='utf-8') as f:\n",
    "        word_to_idx = json.load(f)\n",
    "    with open(f\"data/{task}/{lang}/processed/idx_to_word.json\", 'r', encoding='utf-8') as f:\n",
    "        idx_to_word = json.load(f)\n",
    "    train_path = f\"data/{task}/{lang}/processed/train_data.pth\"\n",
    "    val_path = f\"data/{task}/{lang}/processed/val_data.pth\"\n",
    "    test_path = f\"data/{task}/{lang}/processed/test_data.pth\"\n",
    "else:\n",
    "    tgt_to_idx = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/{task}_to_idx.json\"))\n",
    "    idx_to_tgt = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/idx_to_{task}.json\"))\n",
    "    word_to_idx = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/word_to_idx.json\"))\n",
    "    idx_to_word = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/idx_to_word.json\"))\n",
    "    train_path = f\"data/{task}/processed/{prop_to_prob[prop]}/train_data.pth\"\n",
    "    val_path = f\"data/{task}/processed/{prop_to_prob[prop]}/val_data.pth\"\n",
    "    test_path = f\"data/{task}/processed/{prop_to_prob[prop]}/test_data.pth\"\n",
    "train_data = torch.load(train_path)\n",
    "val_data = torch.load(val_path)\n",
    "test_data = torch.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb9a8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 36, 88)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freqs = Counter()\n",
    "tag_freqs = Counter()\n",
    "\n",
    "for sentence in train_data['words']:\n",
    "    for word in sentence:\n",
    "        if word != word_to_idx['<PAD>']:\n",
    "            word_string = idx_to_word[str(word)]\n",
    "            word_freqs[word_string] += 1\n",
    "\n",
    "for sentence in train_data['tags']:\n",
    "    for tag in sentence:\n",
    "        if tag != tgt_to_idx['<PAD>'] and tag != tgt_to_idx['<START>']:\n",
    "            tag_string = idx_to_tgt[str(tag)]\n",
    "            tag_freqs[tag_string] += 1\n",
    "\n",
    "frequent_tags = [tag for tag, freq in tag_freqs.items() if freq >= 100]\n",
    "common_tags = [tag for tag, freq in tag_freqs.items() if freq >= 10 and freq < 100]\n",
    "rare_tags = [tag for tag, freq in tag_freqs.items() if freq < 10]\n",
    "\n",
    "len(frequent_tags), len(common_tags), len(rare_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4691cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds_frequent = []\n",
    "TI_labs_frequent = [[] for _ in range(len(runs))]\n",
    "M2C_labs_frequent = [[] for _ in range(len(runs))]\n",
    "golds_common = []\n",
    "TI_labs_common = [[] for _ in range(len(runs))]\n",
    "M2C_labs_common = [[] for _ in range(len(runs))]\n",
    "golds_rare = []\n",
    "TI_labs_rare = [[] for _ in range(len(runs))]\n",
    "M2C_labs_rare = [[] for _ in range(len(runs))]\n",
    "\n",
    "for run in runs:\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if run == \"Run1\":\n",
    "                if gold in frequent_tags:\n",
    "                    golds_frequent.append(gold)\n",
    "                elif gold in common_tags:\n",
    "                    golds_common.append(gold)\n",
    "                elif gold in rare_tags:\n",
    "                    golds_rare.append(gold)\n",
    "            if gold in frequent_tags:\n",
    "                TI_labs_frequent[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_frequent[runs.index(run)].append(m2c_pred)\n",
    "            elif gold in common_tags:\n",
    "                TI_labs_common[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_common[runs.index(run)].append(m2c_pred)\n",
    "            elif gold in rare_tags:\n",
    "                TI_labs_rare[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_rare[runs.index(run)].append(m2c_pred)\n",
    "golds_frequent = np.array(golds_frequent)\n",
    "for run in runs:\n",
    "    TI_labs_frequent[runs.index(run)] = np.array(TI_labs_frequent[runs.index(run)])\n",
    "    M2C_labs_frequent[runs.index(run)] = np.array(M2C_labs_frequent[runs.index(run)])\n",
    "golds_common = np.array(golds_common)\n",
    "for run in runs:\n",
    "    TI_labs_common[runs.index(run)] = np.array(TI_labs_common[runs.index(run)])\n",
    "    M2C_labs_common[runs.index(run)] = np.array(M2C_labs_common[runs.index(run)])\n",
    "golds_rare = np.array(golds_rare)\n",
    "for run in runs:\n",
    "    TI_labs_rare[runs.index(run)] = np.array(TI_labs_rare[runs.index(run)])\n",
    "    M2C_labs_rare[runs.index(run)] = np.array(M2C_labs_rare[runs.index(run)])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af91acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy for Frequent tags in Run1: -0.0073\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run1: -0.0130\n",
      "P-value: 0.1270\n",
      "Observed difference in accuracy for Rare tags in Run1: 0.3333\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Frequent tags in Run2: -0.0233\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run2: 0.0000\n",
      "P-value: 1.0000\n",
      "Observed difference in accuracy for Rare tags in Run2: 0.3590\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Frequent tags in Run3: -0.0122\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run3: -0.0065\n",
      "P-value: 0.7110\n",
      "Observed difference in accuracy for Rare tags in Run3: 0.3077\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(runs)):\n",
    "    for freq in [\"Frequent\", \"Common\", \"Rare\"]:\n",
    "        if freq == \"Frequent\":\n",
    "            observed_diff, p_value = permutation_test(golds_frequent, TI_labs_frequent[i], M2C_labs_frequent[i], n_iterations=1000)\n",
    "        elif freq == \"Common\":\n",
    "            observed_diff, p_value = permutation_test(golds_common, TI_labs_common[i], M2C_labs_common[i], n_iterations=1000)\n",
    "        elif freq == \"Rare\":\n",
    "            observed_diff, p_value = permutation_test(golds_rare, TI_labs_rare[i], M2C_labs_rare[i], n_iterations=1000)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Observed difference in accuracy for {freq} tags in {runs[i]}: {observed_diff:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7115e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction for rare tag and M2C: (S[dcl]\\S[wq])\\NP\n",
      "Correct prediction for rare tag and M2C: S[as]/S[poss]\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\NP[thr])/(S[to]\\NP)\n",
      "Correct prediction for rare tag and M2C: ((S[dcl]\\S[dcl])\\NP)/PP\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\NP[thr])/(S[to]\\NP)\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\S[qem])/PP\n"
     ]
    }
   ],
   "source": [
    "for i, lab in enumerate(golds_rare):\n",
    "    if lab == TI_labs_rare[0][i]:\n",
    "        print(f\"Correct prediction for rare tag and TI: {lab}\")\n",
    "    elif lab == M2C_labs_rare[0][i]:\n",
    "        print(f\"Correct prediction for rare tag and M2C: {lab}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
