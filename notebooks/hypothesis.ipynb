{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031242d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_p_value(p1, p2, n):\n",
    "    SE = ((p1*(1-p1)/n) + (p2*(1-p2)/n))**0.5\n",
    "    z = (p2-p1)/SE\n",
    "    p = 2*(1-norm.cdf(abs(z)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bd31e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5973713819971624)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_p_value(0.9291, 0.9350, 1015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87360c0c",
   "metadata": {},
   "source": [
    "# Wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18c11696",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"PMB\"\n",
    "prop = \"1\"\n",
    "prop_to_prob ={\"1\": \"100%\", \"0.5\": \"50%\", \"0.1\": \"10%\", \"0.25\": \"25%\"}\n",
    "model_to_compare = \"BERT_Encoder\"\n",
    "lang = \"it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6eb4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# go to parent directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b8c92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the files\n",
    "TI_preds = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "M2C_preds = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "TI_accs = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "M2C_accs = {\"Run1\": [], \"Run2\": [], \"Run3\": []}\n",
    "\n",
    "runs = [\"Run1\", \"Run2\", \"Run3\"]\n",
    "\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    TI_file = f\"predictions/{task}/predictions_TagInsert_{lang}_{prop}_{run}.csv\"\n",
    "    m2c_file = f\"predictions/{task}/predictions_{model_to_compare}_{lang}_{prop}_{run}.csv\"\n",
    "    with open(TI_file) as f:\n",
    "        # get number of lines in the file\n",
    "        rows = sum(1 for line in f)\n",
    "    with open(TI_file) as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            sentence = []\n",
    "            preds = line.strip().split(\", \")\n",
    "            # remove the last comma\n",
    "            # preds[-1] = preds[-1][:-1]\n",
    "            for j, pred in enumerate(preds):\n",
    "                if j == len(preds) - 1 and not pred.startswith(\"Sentence\"):\n",
    "                    # remove the last comma\n",
    "                    pred = pred[:-1]\n",
    "                if pred.startswith(\"Sentence\"):\n",
    "                    TI_accs[run].append(pred.split(\": \")[1])\n",
    "                elif i != rows - 1:\n",
    "                    word, gold, prediction, order = pred.split(\"|\")\n",
    "                    sentence.append((word, gold, prediction, order))\n",
    "            if len(sentence) != 0:\n",
    "                TI_preds[run].append(sentence)\n",
    "\n",
    "    with open(m2c_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sentence = []\n",
    "            preds = line.strip().split(\", \")\n",
    "            # remove the last comma\n",
    "            # preds[-1] = preds[-1][:-1]\n",
    "            for j, pred in enumerate(preds):\n",
    "                if j == len(preds) - 1 and not pred.startswith(\"Sentence\") and model_to_compare != \"BERT_Encoder\":\n",
    "                    # remove the last comma\n",
    "                    pred = pred[:-1]\n",
    "                if pred.startswith(\"Sentence\"):\n",
    "                    M2C_accs[run].append(pred.split(\": \")[1])\n",
    "                elif i != rows - 1:\n",
    "                    word, gold, prediction = pred.split(\"|\")\n",
    "                    sentence.append((word, gold, prediction))\n",
    "            if len(sentence) != 0:\n",
    "                M2C_preds[run].append(sentence)\n",
    "\n",
    "# convert all the accuracies to floats\n",
    "for run in runs:\n",
    "    TI_accs[run] = [float(acc) for acc in TI_accs[run]]\n",
    "    M2C_accs[run] = [float(acc) for acc in M2C_accs[run]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b4f474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949 944 1015 0.9349753694581281 0.9300492610837439\n",
      "938 943 1015 0.9241379310344827 0.929064039408867\n",
      "945 944 1015 0.9310344827586207 0.9300492610837439\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    correct_ti = 0\n",
    "    correct_m2c = 0\n",
    "    total = 0\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        sentence_len = len(ti_preds)\n",
    "        total += sentence_len\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if ti_pred == gold:\n",
    "                correct_ti += 1\n",
    "            if m2c_pred == gold:\n",
    "                correct_m2c += 1\n",
    "\n",
    "    print(correct_ti, correct_m2c, total, correct_ti / total, correct_m2c / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98b73f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon statistic: 1882.0\n",
      "P-value: 0.3108048407590046\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "TI_runs = np.zeros((total, len(runs)), dtype=int)\n",
    "M2C_runs = np.zeros((total, len(runs)), dtype=int)\n",
    "for run in runs:\n",
    "    run_id = 0\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if ti_pred == gold:\n",
    "                TI_runs[run_id, runs.index(run)] = 1\n",
    "            else:\n",
    "                TI_runs[run_id, runs.index(run)] = 0\n",
    "            if m2c_pred == gold:\n",
    "                M2C_runs[run_id, runs.index(run)] = 1\n",
    "            else:\n",
    "                M2C_runs[run_id, runs.index(run)] = 0\n",
    "            run_id += 1\n",
    "            \n",
    "\n",
    "# Step 1: Average accuracy per test instance across the 3 runs\n",
    "model_a_mean = TI_runs.mean(axis=1)\n",
    "model_b_mean = M2C_runs.mean(axis=1)\n",
    "\n",
    "# Step 2: Compute per-sample difference\n",
    "diffs = model_a_mean - model_b_mean\n",
    "\n",
    "# Step 3: Run Wilcoxon signed-rank test\n",
    "stat, p_value = wilcoxon(diffs)\n",
    "\n",
    "print(f\"Wilcoxon statistic: {stat}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ca50c",
   "metadata": {},
   "source": [
    "# Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4e5c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def permutation_test_multiple_runs(y_true, y_preds_1, y_preds_2, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to compare the accuracy of two models over multiple runs on the same dataset.\n",
    "    \n",
    "    Args:\n",
    "    - y_true (array-like): The true labels (ground truth) for the test set.\n",
    "    - y_preds_1 (array-like): A list of arrays where each array contains the predictions of model 1 for each run.\n",
    "    - y_preds_2 (array-like): A list of arrays where each array contains the predictions of model 2 for each run.\n",
    "    - n_iterations (int): The number of permutations to perform (default is 1000).\n",
    "    \n",
    "    Returns:\n",
    "    - observed_diff (float): The observed difference in accuracy between the two models.\n",
    "    - p_value (float): The p-value of the permutation test, representing the probability\n",
    "      of observing a difference as extreme as the observed one under the null hypothesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the accuracy of each model over all runs\n",
    "    accuracies_1 = [np.mean(y_pred == y_true) for y_pred in y_preds_1]\n",
    "    accuracies_2 = [np.mean(y_pred == y_true) for y_pred in y_preds_2]\n",
    "    \n",
    "    # Calculate the observed difference in accuracy between the two models\n",
    "    observed_acc_1 = np.mean(accuracies_1)  # Mean accuracy of model 1 across all runs\n",
    "    observed_acc_2 = np.mean(accuracies_2)  # Mean accuracy of model 2 across all runs\n",
    "    observed_diff = observed_acc_1 - observed_acc_2  # The observed difference in accuracy\n",
    "    \n",
    "    count = 0  # To track how many times the difference from permutation is as extreme as the observed difference\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Shuffle the true labels randomly\n",
    "        y_true_shuffled = shuffle(y_true)\n",
    "        \n",
    "        # Calculate the accuracy for both models on the shuffled labels for each run\n",
    "        accuracies_1_shuffled = [np.mean(y_pred == y_true_shuffled) for y_pred in y_preds_1]\n",
    "        accuracies_2_shuffled = [np.mean(y_pred == y_true_shuffled) for y_pred in y_preds_2]\n",
    "        \n",
    "        # Calculate the difference in accuracy after shuffling\n",
    "        diff_shuffled = np.mean(accuracies_1_shuffled) - np.mean(accuracies_2_shuffled)\n",
    "        \n",
    "        # If the shuffled difference is as extreme as or more extreme than the observed difference, count it\n",
    "        if np.abs(diff_shuffled) >= np.abs(observed_diff):\n",
    "            count += 1\n",
    "    # Calculate the p-value as the proportion of times the shuffled difference was as extreme as the observed difference\n",
    "    p_value = count / n_iterations\n",
    "    \n",
    "    return observed_diff, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee4ec1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds = []\n",
    "TI_labs = [[] for _ in range(len(runs))]\n",
    "M2C_labs = [[] for _ in range(len(runs))]\n",
    "for run in runs:\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if run == \"Run1\":\n",
    "                golds.append(gold)\n",
    "            TI_labs[runs.index(run)].append(ti_pred)\n",
    "            M2C_labs[runs.index(run)].append(m2c_pred)\n",
    "golds = np.array(golds)\n",
    "for run in runs:\n",
    "    TI_labs[runs.index(run)] = np.array(TI_labs[runs.index(run)])\n",
    "    M2C_labs[runs.index(run)] = np.array(M2C_labs[runs.index(run)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1873f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy: 0.0003\n",
      "P-value: 0.7080\n"
     ]
    }
   ],
   "source": [
    "observed_diff, p_value = permutation_test_multiple_runs(golds, TI_labs, M2C_labs, n_iterations=1000)\n",
    "\n",
    "# Print results\n",
    "print(f\"Observed difference in accuracy: {observed_diff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(y_true, y_pred_1, y_pred_2, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to compare the accuracy of two models on the same dataset.\n",
    "\n",
    "    Args:\n",
    "    - y_true (array-like): The true labels (ground truth) for the test set.\n",
    "    - y_pred_1 (array-like): The predictions made by the first model.\n",
    "    - y_pred_2 (array-like): The predictions made by the second model.\n",
    "    - n_iterations (int): The number of permutations to perform (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "    - observed_diff (float): The observed difference in accuracy between the two models.\n",
    "    - p_value (float): The p-value of the permutation test, representing the probability\n",
    "      of observing a difference as extreme as the observed one under the null hypothesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the observed difference in accuracy between the two models\n",
    "    observed_acc_1 = np.mean(y_pred_1 == y_true)  # Accuracy of model 1\n",
    "    observed_acc_2 = np.mean(y_pred_2 == y_true)  # Accuracy of model 2\n",
    "    observed_diff = observed_acc_1 - observed_acc_2  # The observed difference in accuracy\n",
    "    \n",
    "    count = 0  # To track how many times the difference from permutation is as extreme as the observed difference\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Shuffle the true labels randomly\n",
    "        y_true_shuffled = shuffle(y_true)\n",
    "        \n",
    "        # Calculate the accuracy for both models on the shuffled labels\n",
    "        acc_1_shuffled = np.mean(y_pred_1 == y_true_shuffled)\n",
    "        acc_2_shuffled = np.mean(y_pred_2 == y_true_shuffled)\n",
    "        \n",
    "        # Calculate the difference in accuracy after shuffling\n",
    "        diff_shuffled = acc_1_shuffled - acc_2_shuffled\n",
    "        \n",
    "        # If the shuffled difference is as extreme as or more extreme than the observed difference, count it\n",
    "        if np.abs(diff_shuffled) >= np.abs(observed_diff):\n",
    "            count += 1\n",
    "    \n",
    "    # Calculate the p-value as the proportion of times the shuffled difference was as extreme as the observed difference\n",
    "    p_value = count / n_iterations\n",
    "    \n",
    "    return observed_diff, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeca936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy: 0.0049\n",
      "P-value: 0.0119\n",
      "Observed difference in accuracy: -0.0049\n",
      "P-value: 0.0203\n",
      "Observed difference in accuracy: 0.0010\n",
      "P-value: 0.7295\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(runs)):\n",
    "    observed_diff, p_value = permutation_test(golds, TI_labs[i], M2C_labs[i], n_iterations=1000)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Observed difference in accuracy: {observed_diff:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2bf7b",
   "metadata": {},
   "source": [
    "# Long Tail Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "\n",
    "if task == \"PMB\":\n",
    "    tgt_to_idx = json.load(open(f\"data/{task}/{lang}/processed/{task}_to_idx.json\"))\n",
    "    idx_to_tgt = json.load(open(f\"data/{task}/{lang}/processed/idx_to_{task}.json\"))\n",
    "    with open(f\"data/{task}/{lang}/processed/word_to_idx.json\", 'r', encoding='utf-8') as f:\n",
    "        word_to_idx = json.load(f)\n",
    "    with open(f\"data/{task}/{lang}/processed/idx_to_word.json\", 'r', encoding='utf-8') as f:\n",
    "        idx_to_word = json.load(f)\n",
    "    train_path = f\"data/{task}/{lang}/processed/train_data.pth\"\n",
    "    val_path = f\"data/{task}/{lang}/processed/val_data.pth\"\n",
    "    test_path = f\"data/{task}/{lang}/processed/test_data.pth\"\n",
    "else:\n",
    "    tgt_to_idx = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/{task}_to_idx.json\"))\n",
    "    idx_to_tgt = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/idx_to_{task}.json\"))\n",
    "    word_to_idx = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/word_to_idx.json\"))\n",
    "    idx_to_word = json.load(open(f\"data/{task}/processed/{prop_to_prob[prop]}/idx_to_word.json\"))\n",
    "    train_path = f\"data/{task}/processed/{prop_to_prob[prop]}/train_data.pth\"\n",
    "    val_path = f\"data/{task}/processed/{prop_to_prob[prop]}/val_data.pth\"\n",
    "    test_path = f\"data/{task}/processed/{prop_to_prob[prop]}/test_data.pth\"\n",
    "train_data = torch.load(train_path)\n",
    "val_data = torch.load(val_path)\n",
    "test_data = torch.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb9a8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 36, 88)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freqs = Counter()\n",
    "tag_freqs = Counter()\n",
    "\n",
    "for sentence in train_data['words']:\n",
    "    for word in sentence:\n",
    "        if word != word_to_idx['<PAD>']:\n",
    "            word_string = idx_to_word[str(word)]\n",
    "            word_freqs[word_string] += 1\n",
    "\n",
    "for sentence in train_data['tags']:\n",
    "    for tag in sentence:\n",
    "        if tag != tgt_to_idx['<PAD>'] and tag != tgt_to_idx['<START>']:\n",
    "            tag_string = idx_to_tgt[str(tag)]\n",
    "            tag_freqs[tag_string] += 1\n",
    "\n",
    "frequent_tags = [tag for tag, freq in tag_freqs.items() if freq >= 100]\n",
    "common_tags = [tag for tag, freq in tag_freqs.items() if freq >= 10 and freq < 100]\n",
    "rare_tags = [tag for tag, freq in tag_freqs.items() if freq < 10]\n",
    "\n",
    "len(frequent_tags), len(common_tags), len(rare_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4691cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds_frequent = []\n",
    "TI_labs_frequent = [[] for _ in range(len(runs))]\n",
    "M2C_labs_frequent = [[] for _ in range(len(runs))]\n",
    "golds_common = []\n",
    "TI_labs_common = [[] for _ in range(len(runs))]\n",
    "M2C_labs_common = [[] for _ in range(len(runs))]\n",
    "golds_rare = []\n",
    "TI_labs_rare = [[] for _ in range(len(runs))]\n",
    "M2C_labs_rare = [[] for _ in range(len(runs))]\n",
    "\n",
    "for run in runs:\n",
    "    for ti_preds, m2c_preds, ti_accs, m2c_accs in zip(TI_preds[run], M2C_preds[run], TI_accs[run], M2C_accs[run]):\n",
    "        for i, (ti, m2c) in enumerate(zip(ti_preds, m2c_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            m2c_word, _, m2c_pred = m2c\n",
    "            if run == \"Run1\":\n",
    "                if gold in frequent_tags:\n",
    "                    golds_frequent.append(gold)\n",
    "                elif gold in common_tags:\n",
    "                    golds_common.append(gold)\n",
    "                elif gold in rare_tags:\n",
    "                    golds_rare.append(gold)\n",
    "            if gold in frequent_tags:\n",
    "                TI_labs_frequent[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_frequent[runs.index(run)].append(m2c_pred)\n",
    "            elif gold in common_tags:\n",
    "                TI_labs_common[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_common[runs.index(run)].append(m2c_pred)\n",
    "            elif gold in rare_tags:\n",
    "                TI_labs_rare[runs.index(run)].append(ti_pred)\n",
    "                M2C_labs_rare[runs.index(run)].append(m2c_pred)\n",
    "golds_frequent = np.array(golds_frequent)\n",
    "for run in runs:\n",
    "    TI_labs_frequent[runs.index(run)] = np.array(TI_labs_frequent[runs.index(run)])\n",
    "    M2C_labs_frequent[runs.index(run)] = np.array(M2C_labs_frequent[runs.index(run)])\n",
    "golds_common = np.array(golds_common)\n",
    "for run in runs:\n",
    "    TI_labs_common[runs.index(run)] = np.array(TI_labs_common[runs.index(run)])\n",
    "    M2C_labs_common[runs.index(run)] = np.array(M2C_labs_common[runs.index(run)])\n",
    "golds_rare = np.array(golds_rare)\n",
    "for run in runs:\n",
    "    TI_labs_rare[runs.index(run)] = np.array(TI_labs_rare[runs.index(run)])\n",
    "    M2C_labs_rare[runs.index(run)] = np.array(M2C_labs_rare[runs.index(run)])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af91acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in accuracy for Frequent tags in Run1: -0.0073\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run1: -0.0130\n",
      "P-value: 0.1270\n",
      "Observed difference in accuracy for Rare tags in Run1: 0.3333\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Frequent tags in Run2: -0.0233\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run2: 0.0000\n",
      "P-value: 1.0000\n",
      "Observed difference in accuracy for Rare tags in Run2: 0.3590\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Frequent tags in Run3: -0.0122\n",
      "P-value: 0.0000\n",
      "Observed difference in accuracy for Common tags in Run3: -0.0065\n",
      "P-value: 0.7110\n",
      "Observed difference in accuracy for Rare tags in Run3: 0.3077\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(runs)):\n",
    "    for freq in [\"Frequent\", \"Common\", \"Rare\"]:\n",
    "        if freq == \"Frequent\":\n",
    "            observed_diff, p_value = permutation_test(golds_frequent, TI_labs_frequent[i], M2C_labs_frequent[i], n_iterations=1000)\n",
    "        elif freq == \"Common\":\n",
    "            observed_diff, p_value = permutation_test(golds_common, TI_labs_common[i], M2C_labs_common[i], n_iterations=1000)\n",
    "        elif freq == \"Rare\":\n",
    "            observed_diff, p_value = permutation_test(golds_rare, TI_labs_rare[i], M2C_labs_rare[i], n_iterations=1000)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Observed difference in accuracy for {freq} tags in {runs[i]}: {observed_diff:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7115e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction for rare tag and M2C: (S[dcl]\\S[wq])\\NP\n",
      "Correct prediction for rare tag and M2C: S[as]/S[poss]\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\NP[thr])/(S[to]\\NP)\n",
      "Correct prediction for rare tag and M2C: ((S[dcl]\\S[dcl])\\NP)/PP\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\NP[thr])/(S[to]\\NP)\n",
      "Correct prediction for rare tag and M2C: (S[dcl]\\S[qem])/PP\n"
     ]
    }
   ],
   "source": [
    "for i, lab in enumerate(golds_rare):\n",
    "    if lab == TI_labs_rare[0][i]:\n",
    "        print(f\"Correct prediction for rare tag and TI: {lab}\")\n",
    "    elif lab == M2C_labs_rare[0][i]:\n",
    "        print(f\"Correct prediction for rare tag and M2C: {lab}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
