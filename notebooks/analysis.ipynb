{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TI_file = f\"predictions/CCG/predictions_TagInsert_1_Run1_Epoch9.csv\"\n",
    "TIL2R_file = f\"predictions/CCG/predictions_TagInsertL2R_en_1_Run1_19.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# go to parent directory\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the files\n",
    "TI_preds = []\n",
    "TI_accs = []\n",
    "TIL2R_preds = []\n",
    "TIL2R_accs = []\n",
    "\n",
    "with open(TI_file) as f:\n",
    "    # get number of lines in the file\n",
    "    rows = sum(1 for line in f)\n",
    "with open(TI_file) as f:\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        sentence = []\n",
    "        preds = line.strip().split(\", \")\n",
    "        # remove the last comma\n",
    "        preds[-1] = preds[-1][:-1]\n",
    "        for j, pred in enumerate(preds):\n",
    "            if pred.startswith(\"Sentence\"):\n",
    "                TI_accs.append(pred.split(\": \")[1])\n",
    "            elif i != rows - 1:\n",
    "                word, gold, prediction, order = pred.split(\"|\")\n",
    "                sentence.append((word, gold, prediction, order))\n",
    "        if len(sentence) != 0:\n",
    "            TI_preds.append(sentence)\n",
    "\n",
    "with open(TIL2R_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sentence = []\n",
    "        preds = line.strip().split(\", \")\n",
    "        # remove the last comma\n",
    "        preds[-1] = preds[-1][:-1]\n",
    "        for j, pred in enumerate(preds):\n",
    "            if pred.startswith(\"Sentence\"):\n",
    "                TIL2R_accs.append(pred.split(\": \")[1])\n",
    "            elif i != rows - 1:\n",
    "                word, gold, prediction = pred.split(\"|\")\n",
    "                sentence.append((word, gold, prediction))\n",
    "        if len(sentence) != 0:\n",
    "            TIL2R_preds.append(sentence)\n",
    "        \n",
    "TI_accs = [float(acc) for acc in TI_accs]\n",
    "TIL2R_accs = [float(acc) for acc in TIL2R_accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1910, 1910, 1910, 1910)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TI_accs), len(TIL2R_accs), len(TI_preds), len(TIL2R_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806, 0.8270463941985158, 0.49267207081379893)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_orders = []\n",
    "normalized_indeces = []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    if ti_accs > til2r_accs:\n",
    "        sentence_len = len(ti_preds)\n",
    "        for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            til2r_word, _, til2r_pred = til2r\n",
    "            if ti_pred == gold and til2r_pred != gold:\n",
    "                normalized_index = float(i) / sentence_len\n",
    "                normalized_indeces.append(normalized_index)\n",
    "                normalized_order = float(ti_order) / sentence_len\n",
    "                normalized_orders.append(normalized_order)\n",
    "\n",
    "len(normalized_orders), sum(normalized_orders) / len(normalized_orders), sum(normalized_indeces) / len(normalized_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633, 0.9286839210157678, 0.47513360471537947)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_orders = []\n",
    "normalized_indeces = []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    if ti_accs < til2r_accs:\n",
    "        sentence_len = len(ti_preds)\n",
    "        for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "            ti_word, gold, ti_pred, ti_order = ti\n",
    "            til2r_word, _, til2r_pred = til2r\n",
    "            if til2r_pred == gold and ti_pred != gold:\n",
    "                normalized_index = float(i) / sentence_len\n",
    "                normalized_indeces.append(normalized_index)\n",
    "                normalized_order = float(ti_order) / sentence_len\n",
    "                normalized_orders.append(normalized_order)\n",
    "\n",
    "len(normalized_orders), sum(normalized_orders) / len(normalized_orders), sum(normalized_indeces) / len(normalized_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bais_\\AppData\\Local\\Temp\\ipykernel_31056\\1701371993.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_path)\n",
      "C:\\Users\\bais_\\AppData\\Local\\Temp\\ipykernel_31056\\1701371993.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(val_path)\n",
      "C:\\Users\\bais_\\AppData\\Local\\Temp\\ipykernel_31056\\1701371993.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(test_path)\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "\n",
    "tgt_to_idx = json.load(open(\"data/CCG/processed/10%/CCG_to_idx.json\"))\n",
    "idx_to_tgt = json.load(open(\"data/CCG/processed/10%/idx_to_CCG.json\"))\n",
    "word_to_idx = json.load(open(\"data/CCG/processed/10%/word_to_idx.json\"))\n",
    "idx_to_word = json.load(open(\"data/CCG/processed/10%/idx_to_word.json\"))\n",
    "train_path = \"data/CCG/processed/100%/train_data.pth\"\n",
    "val_path = \"data/CCG/processed/100%/val_data.pth\"\n",
    "test_path = \"data/CCG/processed/100%/test_data.pth\"\n",
    "train_data = torch.load(train_path)\n",
    "val_data = torch.load(val_path)\n",
    "test_data = torch.load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(',', 52749),\n",
       "  ('the', 44759),\n",
       "  ('.', 42360),\n",
       "  ('of', 24837),\n",
       "  ('to', 24046),\n",
       "  ('a', 20848),\n",
       "  ('and', 17264),\n",
       "  ('in', 16437),\n",
       "  (\"'s\", 10145),\n",
       "  ('that', 8650)],\n",
       " [('N', 182074),\n",
       "  ('N/N', 151657),\n",
       "  ('NP/N', 83736),\n",
       "  ('PP/NP', 56438),\n",
       "  (',', 52749),\n",
       "  ('.', 42849),\n",
       "  ('N/PP', 37774),\n",
       "  ('conj', 21975),\n",
       "  ('(N/N)/(N/N)', 20117),\n",
       "  ('NP', 19969)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freqs = Counter()\n",
    "tag_freqs = Counter()\n",
    "\n",
    "for sentence in train_data['words']:\n",
    "    for word in sentence:\n",
    "        if word != word_to_idx['<PAD>']:\n",
    "            word_string = idx_to_word[str(word)]\n",
    "            word_freqs[word_string] += 1\n",
    "\n",
    "for sentence in train_data['tags']:\n",
    "    for tag in sentence:\n",
    "        if tag != tgt_to_idx['<PAD>'] and tag != tgt_to_idx['<START>']:\n",
    "            tag_string = idx_to_tgt[str(tag)]\n",
    "            tag_freqs[tag_string] += 1\n",
    "\n",
    "word_freqs.most_common(10), tag_freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unseen words: 1618\n",
      "TI accuracy on unseen words: 0.9276885043263288\n",
      "TIL2R accuracy on unseen words: 0.9252163164400494\n",
      "Average normalized order of unseen words: 0.5993527893477102\n",
      "Average normalized index of unseen words: 0.45067907892956977\n"
     ]
    }
   ],
   "source": [
    "normalized_orders = []\n",
    "normalized_indeces = []\n",
    "ti_unseen_words_accs = []\n",
    "til2r_unseen_words_accs = []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    # calculate accuracy of the two models on unseen words\n",
    "    for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "        ti_word, gold, ti_pred, ti_order = ti\n",
    "        til2r_word, _, til2r_pred = til2r\n",
    "        if ti_word not in word_freqs:\n",
    "            ti_unseen_words_accs.append(ti_pred == gold)\n",
    "            normalized_order = float(ti_order) / len(ti_preds)\n",
    "            normalized_orders.append(normalized_order)\n",
    "            til2r_unseen_words_accs.append(til2r_pred == gold)\n",
    "            normalized_index = float(i) / len(ti_preds)\n",
    "            normalized_indeces.append(normalized_index)\n",
    "\n",
    "print(f\"Number of unseen words: {len(ti_unseen_words_accs)}\")\n",
    "print(f\"TI accuracy on unseen words: {sum(ti_unseen_words_accs) / len(ti_unseen_words_accs)}\")\n",
    "print(f\"TIL2R accuracy on unseen words: {sum(til2r_unseen_words_accs) / len(til2r_unseen_words_accs)}\")\n",
    "print(f\"Average normalized order of unseen words: {sum(normalized_orders) / len(normalized_orders)}\")\n",
    "print(f\"Average normalized index of unseen words: {sum(normalized_indeces) / len(normalized_indeces)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 321, 1089)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_tags = [tag for tag, freq in tag_freqs.items() if freq >= 100]\n",
    "common_tags = [tag for tag, freq in tag_freqs.items() if freq >= 10 and freq < 100]\n",
    "rare_tags = [tag for tag, freq in tag_freqs.items() if freq < 10]\n",
    "\n",
    "len(frequent_tags), len(common_tags), len(rare_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent tags: 45191\n",
      "TI accuracy on frequent tags: 0.9480870084751388\n",
      "TIL2R accuracy on frequent tags: 0.9430638844017614\n",
      "Average normalized order of frequent tags: 0.51645422689856\n",
      "Average normalized index of frequent tags: 0.4800486356783123\n",
      "Number of common tags: 428\n",
      "TI accuracy on common tags: 0.4696261682242991\n",
      "TIL2R accuracy on common tags: 0.5841121495327103\n",
      "Average normalized order of common tags: 0.9028993652917978\n",
      "Average normalized index of common tags: 0.4020208276279044\n",
      "Number of rare tags: 92\n",
      "TI accuracy on rare tags: 0.0\n",
      "TIL2R accuracy on rare tags: 0.21739130434782608\n",
      "Average normalized order of rare tags: 0.8781339183109241\n",
      "Average normalized index of rare tags: 0.3787890212260492\n",
      "Number of OOV tags: 16\n",
      "TI accuracy on OOV tags: 0.0\n",
      "TIL2R accuracy on OOV tags: 0.0\n",
      "Average normalized order of OOV tags: 0.7617364623549031\n",
      "Average normalized index of OOV tags: 0.481787555240467\n"
     ]
    }
   ],
   "source": [
    "normalized_freq_orders, normalized_freq_indeces = [], []\n",
    "normalized_common_orders, normalized_common_indeces = [], []\n",
    "normalized_rare_orders, normalized_rare_indeces = [], []\n",
    "normalized_oov_orders, normalized_oov_indeces = [], []\n",
    "ti_freq_tags_accs, til2r_freq_tags_accs = [], []\n",
    "ti_common_tags_accs, til2r_common_tags_accs = [], []\n",
    "ti_rare_tags_accs, til2r_rare_tags_accs = [], []\n",
    "ti_oov_tags_accs, til2r_oov_tags_accs = [], []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "        ti_word, gold, ti_pred, ti_order = ti\n",
    "        til2r_word, _, til2r_pred = til2r\n",
    "        if gold not in frequent_tags and gold not in common_tags and gold not in rare_tags:\n",
    "            ti_oov_tags_accs.append(ti_pred == gold)\n",
    "            normalized_order = float(ti_order) / len(ti_preds)\n",
    "            normalized_oov_orders.append(normalized_order)\n",
    "            til2r_oov_tags_accs.append(til2r_pred == gold)\n",
    "            normalized_index = float(i) / len(ti_preds)\n",
    "            normalized_oov_indeces.append(normalized_index)\n",
    "        elif gold in frequent_tags:\n",
    "            ti_freq_tags_accs.append(ti_pred == gold)\n",
    "            normalized_order = float(ti_order) / len(ti_preds)\n",
    "            normalized_freq_orders.append(normalized_order)\n",
    "            til2r_freq_tags_accs.append(til2r_pred == gold)\n",
    "            normalized_index = float(i) / len(ti_preds)\n",
    "            normalized_freq_indeces.append(normalized_index)\n",
    "        elif gold in common_tags:\n",
    "            ti_common_tags_accs.append(ti_pred == gold)\n",
    "            normalized_order = float(ti_order) / len(ti_preds)\n",
    "            normalized_common_orders.append(normalized_order)\n",
    "            til2r_common_tags_accs.append(til2r_pred == gold)\n",
    "            normalized_index = float(i) / len(ti_preds)\n",
    "            normalized_common_indeces.append(normalized_index)\n",
    "        elif gold in rare_tags:\n",
    "            ti_rare_tags_accs.append(ti_pred == gold)\n",
    "            normalized_order = float(ti_order) / len(ti_preds)\n",
    "            normalized_rare_orders.append(normalized_order)\n",
    "            til2r_rare_tags_accs.append(til2r_pred == gold)\n",
    "            normalized_index = float(i) / len(ti_preds)\n",
    "            normalized_rare_indeces.append(normalized_index)\n",
    "\n",
    "print(f\"Number of frequent tags: {len(ti_freq_tags_accs)}\")\n",
    "print(f\"TI accuracy on frequent tags: {sum(ti_freq_tags_accs) / len(ti_freq_tags_accs)}\")\n",
    "print(f\"TIL2R accuracy on frequent tags: {sum(til2r_freq_tags_accs) / len(til2r_freq_tags_accs)}\")\n",
    "print(f\"Average normalized order of frequent tags: {sum(normalized_freq_orders) / len(normalized_freq_orders)}\")\n",
    "print(f\"Average normalized index of frequent tags: {sum(normalized_freq_indeces) / len(normalized_freq_indeces)}\")\n",
    "print(f\"Number of common tags: {len(ti_common_tags_accs)}\")\n",
    "print(f\"TI accuracy on common tags: {sum(ti_common_tags_accs) / len(ti_common_tags_accs)}\")\n",
    "print(f\"TIL2R accuracy on common tags: {sum(til2r_common_tags_accs) / len(til2r_common_tags_accs)}\")\n",
    "print(f\"Average normalized order of common tags: {sum(normalized_common_orders) / len(normalized_common_orders)}\")\n",
    "print(f\"Average normalized index of common tags: {sum(normalized_common_indeces) / len(normalized_common_indeces)}\")\n",
    "print(f\"Number of rare tags: {len(ti_rare_tags_accs)}\")\n",
    "print(f\"TI accuracy on rare tags: {sum(ti_rare_tags_accs) / len(ti_rare_tags_accs)}\")\n",
    "print(f\"TIL2R accuracy on rare tags: {sum(til2r_rare_tags_accs) / len(til2r_rare_tags_accs)}\")\n",
    "print(f\"Average normalized order of rare tags: {sum(normalized_rare_orders) / len(normalized_rare_orders)}\")\n",
    "print(f\"Average normalized index of rare tags: {sum(normalized_rare_indeces) / len(normalized_rare_indeces)}\")\n",
    "print(f\"Number of OOV tags: {len(ti_oov_tags_accs)}\")\n",
    "print(f\"TI accuracy on OOV tags: {sum(ti_oov_tags_accs) / len(ti_oov_tags_accs)}\")\n",
    "print(f\"TIL2R accuracy on OOV tags: {sum(til2r_oov_tags_accs) / len(til2r_oov_tags_accs)}\")\n",
    "print(f\"Average normalized order of OOV tags: {sum(normalized_oov_orders) / len(normalized_oov_orders)}\")\n",
    "print(f\"Average normalized index of OOV tags: {sum(normalized_oov_indeces) / len(normalized_oov_indeces)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags where TI is right and TIL2R is wrong: 1007\n",
      "Number of unique tags where TI is right and TIL2R is wrong: 137\n",
      "Average normalized order of words where TI is right and TIL2R is wrong: 0.814929638407935\n",
      "Average normalized index of words where TI is right and TIL2R is wrong: 0.49595146289663095\n",
      "Most common TI preds where TI is right and TIL2R is wrong: [('N', 136), ('N/N', 81), ('PP/NP', 77), ('((S\\\\NP)\\\\(S\\\\NP))/NP', 60), ('N/PP', 60), ('(S[dcl]\\\\NP)/NP', 50), ('(N\\\\N)/NP', 34), ('NP/N', 28), ('(S\\\\NP)\\\\(S\\\\NP)', 25), ('S[adj]\\\\NP', 24)]\n",
      "Most common TIL2R preds where TI is right and TIL2R is wrong: [('N', 105), ('PP/NP', 70), ('N/N', 70), ('N/PP', 59), ('(N\\\\N)/NP', 51), ('(N/N)/(N/N)', 46), ('((S\\\\NP)\\\\(S\\\\NP))/NP', 38), ('((S[dcl]\\\\NP)/PP)/NP', 29), ('S[adj]\\\\NP', 21), ('N\\\\N', 20)]\n"
     ]
    }
   ],
   "source": [
    "normalized_orders = []\n",
    "normalized_indeces = []\n",
    "right_for_ti = []\n",
    "wrong_for_til2r = []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    sentence_len = len(ti_preds)\n",
    "    for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "        ti_word, gold, ti_pred, ti_order = ti\n",
    "        til2r_word, _, til2r_pred = til2r\n",
    "        if ti_pred == gold and til2r_pred != gold:\n",
    "            right_for_ti.append(ti_pred)\n",
    "            wrong_for_til2r.append(til2r_pred)\n",
    "            normalized_index = float(i) / sentence_len\n",
    "            normalized_indeces.append(normalized_index)\n",
    "            normalized_order = float(ti_order) / sentence_len\n",
    "            normalized_orders.append(normalized_order)\n",
    "\n",
    "right_counter = Counter(right_for_ti)\n",
    "wrong_counter = Counter(wrong_for_til2r)\n",
    "\n",
    "print(f\"Number of tags where TI is right and TIL2R is wrong: {len(right_for_ti)}\")\n",
    "print(f\"Number of unique tags where TI is right and TIL2R is wrong: {len(right_counter)}\")\n",
    "print(f\"Average normalized order of words where TI is right and TIL2R is wrong: {sum(normalized_orders) / len(normalized_orders)}\")\n",
    "print(f\"Average normalized index of words where TI is right and TIL2R is wrong: {sum(normalized_indeces) / len(normalized_indeces)}\")\n",
    "print(f\"Most common TI preds where TI is right and TIL2R is wrong: {right_counter.most_common(10)}\")\n",
    "print(f\"Most common TIL2R preds where TI is right and TIL2R is wrong: {wrong_counter.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commas and periods where TI is wrong: 2\n",
      "Number of commas and number of periods where TI is wrong: Counter({',': 1, '.': 1})\n",
      "Average normalized order of commas and periods where TI is wrong: 0.7707122093023255\n",
      "Number of commas and periods where TIL2R is wrong: 21\n",
      "Number of commas and number of periods where TIL2R is wrong: Counter({',': 16, '.': 5})\n",
      "Average normalized index of commas and periods where TIL2R is wrong: 0.6248005321719486\n"
     ]
    }
   ],
   "source": [
    "# count how many times both model get wrong a comma or a period\n",
    "normalized_orders = []\n",
    "normalized_indeces = []\n",
    "ti_wrong_comma_period = []\n",
    "til2r_wrong_comma_period = []\n",
    "for ti_preds, til2r_preds, ti_accs, til2r_accs in zip(TI_preds, TIL2R_preds, TI_accs, TIL2R_accs):\n",
    "    sentence_len = len(ti_preds)\n",
    "    for i, (ti, til2r) in enumerate(zip(ti_preds, til2r_preds)):\n",
    "        ti_word, gold, ti_pred, ti_order = ti\n",
    "        til2r_word, _, til2r_pred = til2r\n",
    "        if gold in [',', '.']:\n",
    "            if ti_pred != gold:\n",
    "                ti_wrong_comma_period.append(gold)\n",
    "                normalized_order = float(ti_order) / sentence_len\n",
    "                normalized_orders.append(normalized_order)\n",
    "            if til2r_pred != gold:\n",
    "                til2r_wrong_comma_period.append(gold)\n",
    "                normalized_index = float(i) / sentence_len\n",
    "                normalized_indeces.append(normalized_index)\n",
    "\n",
    "ti_wrong_comma_period_counter = Counter(ti_wrong_comma_period)\n",
    "til2r_wrong_comma_period_counter = Counter(til2r_wrong_comma_period)\n",
    "\n",
    "print(f\"Number of commas and periods where TI is wrong: {len(ti_wrong_comma_period)}\")\n",
    "print(f\"Number of commas and number of periods where TI is wrong: {ti_wrong_comma_period_counter}\")\n",
    "print(f\"Average normalized order of commas and periods where TI is wrong: {sum(normalized_orders) / len(normalized_orders)}\")\n",
    "print(f\"Number of commas and periods where TIL2R is wrong: {len(til2r_wrong_comma_period)}\")\n",
    "print(f\"Number of commas and number of periods where TIL2R is wrong: {til2r_wrong_comma_period_counter}\")\n",
    "print(f\"Average normalized index of commas and periods where TIL2R is wrong: {sum(normalized_indeces) / len(normalized_indeces)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
